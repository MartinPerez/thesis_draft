\begin{fullwidth}
\chapter{\label{ch:super_syllables}
Evidence for the superposition principle in two-syllabic representations}
\end{fullwidth}

\begin{chabstract}

In this chapter we show the subject specific networks obtained. Analyse superposition decoding results and look into amodal representations.

\end{chabstract}

% notes
% introduction
% review the problem of binding from a connectionist perspective
% review the mechanism of the tensor framework and its operational implications. In particular the substraction of components.


% experimental design
% remember to mention in other sections the risks of the task itself.
% that we are looking for automatic responses and not forcing any
% particular operation that forces unification. Equally emphasizing both
% syllables too in the task to at least force complete processing.


\section{Behavioural performance}
%
The subjects had an almost perfect behavioural performance in both visual and auditory query tasks, except for Subject 05 that reported concentration span issues over all the acquisition.
Note that due to the experimental design structure, in which we only query few random samples, small score decrements can imply distraction over an important task segment.
So we consider all subjects data apt to neuroimaging interpretation, with small caution over Subject 05. Behavioural performance details are provided in Table \ref{table:behaviour}.


\begin{table}
\begin{tabular}{lrrr}
\toprule
{} &  Visual task (\%) &  Auditory task (\%) &  Overall (\%) \\
\midrule
Subject 01 &        97.22 &          97.22 &    97.22 \\
Subject 02 &       100.00 &          98.61 &    99.31 \\
Subject 03 &        97.92 &          97.22 &    97.57 \\
Subject 04 &        99.31 &          99.31 &    99.31 \\
Subject 05 &        92.36 &          88.89 &    90.62 \\
\bottomrule
\end{tabular}
\caption{Behavioural performance on two-syllabic representation task}
\label{table:behaviour}
\end{table}


\section{Subject networks}
%
The anatomical regions 


Depict example subject gray matter, then primary visual, primary auditory and motor regions. Show V1, V2 (blueish), A1, A11, A12 (greeinsh), M (red) colored differently.

Analyze language localizer results for all subjects and modalities. Alongside final dilated gray matter intersected consideration.

Analyze task effect maps, emphasize expected retinotopic effect.

Show map with final 6 regions derived from effect maps and language localizer.


\section{Sensory systems}
%
That includes motor as a good sanity check. remember to have one table for word and first-second and one table for separates. Task decoding checks, motor clic and visual retinotopy? move to sensory? compute V1 vs V2 as special cases? This just shows that the subjects perceived the stimuli and responded to the task directly from the fmri. Do this correspond to behavioral performance in subjects?

Go directly into sanity check validation in task data from left and right clic. Comment confusion matrix in visual retinotopy, in reality is one category that is not well distinguished from the other two. Is is the same for visual and auditory?

Decoding of broad sensory representations. Should keep to rest of the brain and not language. Should be amodal. Is it worth it to separate analysis by sub areas in vision and audition? likely yes. Makes V1, V2, (maybe also, V3, V4) ... A1, A11, A12


\section{Holistic representations}
%notes
% Subject 3 is the only top notch in word classification. Subject 5 follows. All other are doubful (subject 4 slightly) Very dissapointing subject 1
% Looking at the rest, subject 1 has a boost from super manipulation in auditory modality. Visual is barely good and destroyed on super manipulations. In subject 3 the pattern is inversed for visual modality in super manipulations. There seems to be some pattern in super manipulation here. Might benefit from more hardcore super manipulation not implemented. There seems to be some weak first and second decoding from the visual modality as should be the case... Why super works here??? puzzling...
% There are very important chunks completely missed by the metaanalysis ROIs. Only 12 survive ***, 16 ** and 21 *.
% Likely I want to change my analysis to reflect 

Sp

\section{Superposed representations}
%notes
% Interesting pattern in local, with visual and auditory on super.


Here we comment again on retinotopy on V1 as a special case?

local representations

distributed representations




\section{Amodal representations}
%notes
% do not look for anything in lang_holistic
% Bizarre amodal improvement of first and second classification in lang_holistic


\section{Decoding of language in genera vs specific areas. broca? key areas from metanalysis here for a more detailed analysis? Only if it makes sense to go further.}
%
Sp



\section{Introduction -- data-aware feature grouping}
%
Spatially and information-aware compression schemes are probably better suited 
for structured signals --e.g. images-- \citep{Achanta2012}.\footnote{This phenomenon is known as percolation 
in random graphs \citep{stauffer1992}.}.


\begin{figure}[hptb]
\centering
\includegraphics[width=0.7 \linewidth]{figures/part_II/feature_grouping.eps} 
\caption{\textbf{Feature grouping for statistical analysis of structured 
images:} Illustration of the different steps of feature grouping-based data 
approximation .
%
The approach consists in finding a data-driven spatial reduction $\bPhif$ using 
clustering. 
%
Then, the data $\bx$ are reduced to $\bPhif \ \bx$ and then used for further 
statistical analysis (e.g. classification). 
}
\label{fig:feature_grouping}
\end{figure}

\paragraph{Finding a feature grouping matrix:}
We now consider a data-driven approach to build the matrix $\bPhif$.
%
We rely on feat


\section{Existing fast clustering algorithms}
%
K-means clustering is a natural choice as it minimizes the total
inertia


\section{Contributed clustering algorithm -- ReNA}
%
\subsection{Preliminaries: neighbors graphs}
%
For feature clustering on structured signals, an algorithm should take 
advantage of the generative nature of the data, e.g. for images,
work with local image statistics. 
%
Hence we rely on neighborhood graphs \citep{Eppstein1997}.

Neighborhood gra

\subsection{ReNA: algorithm outline}
%
In a nutshell, our algorithm relies on extracting the connect components
